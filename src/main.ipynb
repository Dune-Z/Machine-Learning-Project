{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'preprocess.DropColumns'>\n",
      "<class 'preprocess.HandleSizeMapping'>\n",
      "<class 'preprocess.OrdinalEncoder'>\n",
      "<class 'preprocess.MeanImputer'>\n",
      "<class 'preprocess.ComputeItemVectors'>\n",
      "Optimizing weights and thresholds, round 1\n",
      "Iteration 0: loss = 14.699899673461914\n",
      "Iteration 100: loss = 7.130328178405762\n",
      "Iteration 200: loss = 4.596743583679199\n",
      "Iteration 300: loss = 3.9746382236480713\n",
      "Optimizing item vectors, round 1\n",
      "Iteration 0: loss = 3.9704017639160156\n",
      "Iteration 100: loss = 3.9702088832855225\n",
      "Iteration 200: loss = 3.970015287399292\n",
      "Iteration 300: loss = 3.969823122024536\n",
      "Optimizing weights and thresholds, round 2\n",
      "Iteration 0: loss = 3.9698212146759033\n",
      "Iteration 100: loss = 3.7763686180114746\n",
      "Iteration 200: loss = 3.607821226119995\n",
      "Iteration 300: loss = 3.4580116271972656\n",
      "Optimizing item vectors, round 2\n",
      "Iteration 0: loss = 3.456601142883301\n",
      "Iteration 100: loss = 3.4565646648406982\n",
      "Iteration 200: loss = 3.456528425216675\n",
      "Iteration 300: loss = 3.4564919471740723\n",
      "Optimizing weights and thresholds, round 3\n",
      "Iteration 0: loss = 3.456491470336914\n",
      "Iteration 100: loss = 3.3663270473480225\n",
      "Iteration 200: loss = 3.28352952003479\n",
      "Iteration 300: loss = 3.2078723907470703\n",
      "Optimizing item vectors, round 3\n",
      "Iteration 0: loss = 3.207151174545288\n",
      "Iteration 100: loss = 3.207139253616333\n",
      "Iteration 200: loss = 3.2071280479431152\n",
      "Iteration 300: loss = 3.2071166038513184\n",
      "Optimizing weights and thresholds, round 4\n",
      "Iteration 0: loss = 3.2071166038513184\n",
      "Iteration 100: loss = 3.154956102371216\n",
      "Iteration 200: loss = 3.106520175933838\n",
      "Iteration 300: loss = 3.061654806137085\n",
      "Optimizing item vectors, round 4\n",
      "Iteration 0: loss = 3.0612239837646484\n",
      "Iteration 100: loss = 3.0612192153930664\n",
      "Iteration 200: loss = 3.0612146854400635\n",
      "Iteration 300: loss = 3.0612099170684814\n",
      "Optimizing weights and thresholds, round 5\n",
      "Iteration 0: loss = 3.0612101554870605\n",
      "Iteration 100: loss = 3.027806282043457\n",
      "Iteration 200: loss = 2.996488571166992\n",
      "Iteration 300: loss = 2.9671592712402344\n",
      "Optimizing item vectors, round 5\n",
      "Iteration 0: loss = 2.966876268386841\n",
      "Iteration 100: loss = 2.96687388420105\n",
      "Iteration 200: loss = 2.9668712615966797\n",
      "Iteration 300: loss = 2.9668691158294678\n",
      "Optimizing weights and thresholds, round 6\n",
      "Iteration 0: loss = 2.9668691158294678\n",
      "Iteration 100: loss = 2.943894624710083\n",
      "Iteration 200: loss = 2.922175645828247\n",
      "Iteration 300: loss = 2.9016530513763428\n",
      "Optimizing item vectors, round 6\n",
      "Iteration 0: loss = 2.901453733444214\n",
      "Iteration 100: loss = 2.9014525413513184\n",
      "Iteration 200: loss = 2.9014511108398438\n",
      "Iteration 300: loss = 2.9014499187469482\n",
      "Optimizing weights and thresholds, round 7\n",
      "Iteration 0: loss = 2.901449680328369\n",
      "Iteration 100: loss = 2.8847782611846924\n",
      "Iteration 200: loss = 2.8689069747924805\n",
      "Iteration 300: loss = 2.8537988662719727\n",
      "Optimizing item vectors, round 7\n",
      "Iteration 0: loss = 2.853651762008667\n",
      "Iteration 100: loss = 2.853651285171509\n",
      "Iteration 200: loss = 2.8536500930786133\n",
      "Iteration 300: loss = 2.853649616241455\n",
      "Optimizing weights and thresholds, round 8\n",
      "Iteration 0: loss = 2.853649616241455\n",
      "Iteration 100: loss = 2.841036081314087\n",
      "Iteration 200: loss = 2.8289568424224854\n",
      "Iteration 300: loss = 2.8173890113830566\n",
      "Optimizing item vectors, round 8\n",
      "Iteration 0: loss = 2.8172755241394043\n",
      "Iteration 100: loss = 2.817275047302246\n",
      "Iteration 200: loss = 2.817274808883667\n",
      "Iteration 300: loss = 2.817274332046509\n",
      "Optimizing weights and thresholds, round 9\n",
      "Iteration 0: loss = 2.817274332046509\n",
      "Iteration 100: loss = 2.8074073791503906\n",
      "Iteration 200: loss = 2.7979109287261963\n",
      "Iteration 300: loss = 2.7887706756591797\n",
      "Optimizing item vectors, round 9\n",
      "Iteration 0: loss = 2.7886810302734375\n",
      "Iteration 100: loss = 2.7886807918548584\n",
      "Iteration 200: loss = 2.7886805534362793\n",
      "Iteration 300: loss = 2.7886803150177\n",
      "<class 'preprocess.DropColumns'>\n",
      "<class 'preprocess.OneHotEncoder'>\n",
      "<class 'preprocess.StandardScaler'>\n",
      "<class 'preprocess.MinMaxScaler'>\n",
      "<class 'preprocess.ConstantScaler'>\n",
      "<class 'preprocess.TargetEncoder'>\n",
      "<class 'preprocess.DropColumns'>\n",
      "<class 'preprocess.SelectOutputColumns'>\n",
      "<class 'preprocess.MeanImputer'>\n",
      "<class 'preprocess.MedianImputer'>\n",
      "<class 'preprocess.OneHotEncoder'>\n",
      "<class 'preprocess.AugmentData'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.rcParams['font.family'] = ['serif']\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman']\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "\n",
    "import utils\n",
    "import preprocess\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from utils import fetch_train_data, describe_data, evaluate_model, train_test_split, random_split_aggr\n",
    "from preprocess import *\n",
    "\n",
    "df = fetch_train_data(path='../data/train_data_all_filled.json')\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Preprocess train data\n",
    "prep = Preprocessor()\n",
    "train_df = prep.cleanse(train_df, is_train=True)\n",
    "train_df.dropna(subset=['fit'], inplace=True)\n",
    "\n",
    "prep.pipeline = [\n",
    "    ##\n",
    "    DropColumns(cols=['user_name', 'review', 'review_summary', 'rating']),\n",
    "    HandleSizeMapping(),  # handle size mapping\n",
    "    OrdinalEncoder(cols=['fit', 'item_name', 'cup_size']),  # (necessary)\n",
    "    MeanImputer(\n",
    "        cols=['weight', 'height', 'bust_size', 'cup_size']),  # (necessary)\n",
    "    ComputeItemVectors(),  # compute item vectors\n",
    "    ##\n",
    "    DropColumns(cols=['size_scheme', 'size']),\n",
    "    OneHotEncoder(cols=['size_suffix', 'rented_for', 'body_type']),\n",
    "    StandardScaler(cols=[\n",
    "        'weight', 'height', 'bust_size', 'cup_size', 'item_weight',\n",
    "        'item_height', 'item_bust_size', 'item_cup_size'\n",
    "    ]),\n",
    "    MinMaxScaler(cols=['age', 'price', 'usually_wear']),\n",
    "    ConstantScaler(\n",
    "        cols=[\n",
    "            'age', 'price', 'usually_wear', 'weight', 'height', 'bust_size',\n",
    "            'cup_size', 'item_weight', 'item_height', 'item_bust_size',\n",
    "            'item_cup_size'\n",
    "        ],\n",
    "        value=1e-4\n",
    "    ),  # Multiplying by 1e-4 to downscale the effect of these features\n",
    "    TargetEncoder(cols=['brand', 'category', 'size_main'],\n",
    "                  target_cols=['weight', 'height', 'bust_size', 'cup_size'],\n",
    "                  name='target_encoder'),\n",
    "    DropColumns(cols=['brand', 'category', 'size_main']),\n",
    "    SelectOutputColumns(\n",
    "        target='target_encoder'\n",
    "    ),  # append the output of 'target_encoder' to the input of the next transformer\n",
    "    MeanImputer(cols=['age', 'weight', 'height', 'bust_size', 'cup_size']),\n",
    "    MedianImputer(cols=['price', 'usually_wear']),\n",
    "    OneHotEncoder(cols=['item_name']),\n",
    "    AugmentData(target_cols=['weight', 'height', 'bust_size', 'cup_size'],\n",
    "                ratio_small=0.2,\n",
    "                ratio_large=0.15),\n",
    "]\n",
    "\n",
    "train_df_prep = train_df.copy()\n",
    "train_df_prep = prep.fit_transform(train_df_prep)\n",
    "\n",
    "X_train = train_df_prep.drop(columns=['fit']).to_numpy(dtype=np.float16)\n",
    "y_train = train_df_prep['fit'].to_numpy(dtype=np.float16)\n",
    "\n",
    "# Preprocess test data\n",
    "test_df = prep.cleanse(test_df)\n",
    "test_df.dropna(subset=['fit'], inplace=True)\n",
    "\n",
    "test_df_prep = test_df.copy()\n",
    "test_df_prep = prep.transform(test_df_prep)\n",
    "\n",
    "X_test = test_df_prep.drop(columns=['fit']).to_numpy(dtype=np.float16)\n",
    "y_test = test_df_prep['fit'].to_numpy(dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.061e-05, 7.987e-06, 2.587e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [2.933e-05, 2.682e-06, 3.481e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [1.323e-05, 1.597e-05, 5.728e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        ...,\n",
       "        [2.599e-05, 5.305e-06, 3.260e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [1.919e-05, 2.664e-05, 3.612e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [1.752e-05, 1.067e-05, 3.147e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00]], dtype=float16),\n",
       " (73545, 4135),\n",
       " array([1., 2., 1., ..., 2., 2., 2.], dtype=float16),\n",
       " (73545,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_train.shape, y_train, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.722e-06, 0.000e+00, 2.587e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [1.323e-05, 2.682e-06, 4.381e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [1.150e-05, 1.866e-05, 4.154e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        ...,\n",
       "        [4.470e-06, 5.305e-06, 3.034e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [2.182e-05, 2.134e-05, 2.921e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00],\n",
       "        [1.734e-05, 5.305e-06, 2.021e-05, ..., 0.000e+00, 0.000e+00,\n",
       "         0.000e+00]], dtype=float16),\n",
       " (17553, 4135),\n",
       " array([1., 1., 1., ..., 1., 1., 1.], dtype=float16),\n",
       " (17553,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, X_test.shape, y_test, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1., 2.], dtype=float16), array([11382, 16311, 13231]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.42916303\n",
      "Epoch 3, change: 0.21372357\n",
      "Epoch 4, change: 0.15203898\n",
      "Epoch 5, change: 0.10946079\n",
      "Epoch 6, change: 0.08948606\n",
      "Epoch 7, change: 0.05997324\n",
      "Epoch 8, change: 0.04547160\n",
      "Epoch 9, change: 0.03795874\n",
      "Epoch 10, change: 0.03446740\n",
      "Epoch 11, change: 0.02761528\n",
      "Epoch 12, change: 0.01753339\n",
      "Epoch 13, change: 0.02758795\n",
      "Epoch 14, change: 0.02024626\n",
      "Epoch 15, change: 0.01348880\n",
      "Epoch 16, change: 0.00828954\n",
      "Epoch 17, change: 0.00691025\n",
      "Epoch 18, change: 0.00572338\n",
      "Epoch 19, change: 0.00496604\n",
      "max_iter reached after 51 secondsEpoch 20, change: 0.00426411\n",
      "\n",
      "(array([0., 1., 2.], dtype=float16), array([11382, 16311, 13231]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasined/.pyenv/versions/3.10.8/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   51.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.36676316\n",
      "Epoch 3, change: 0.23528819\n",
      "Epoch 4, change: 0.13489953\n",
      "Epoch 5, change: 0.09846922\n",
      "Epoch 6, change: 0.09314891\n",
      "Epoch 7, change: 0.07150113\n",
      "Epoch 8, change: 0.04654496\n",
      "Epoch 9, change: 0.03272169\n",
      "Epoch 10, change: 0.03172053\n",
      "Epoch 11, change: 0.02656257\n",
      "Epoch 12, change: 0.02443648\n",
      "Epoch 13, change: 0.01872265\n",
      "Epoch 14, change: 0.01522396\n",
      "Epoch 15, change: 0.01244179\n",
      "Epoch 16, change: 0.00915024\n",
      "Epoch 17, change: 0.00799067\n",
      "Epoch 18, change: 0.00599163\n",
      "Epoch 19, change: 0.00496753\n",
      "max_iter reached after 51 secondsEpoch 20, change: 0.00431940\n",
      "\n",
      "(array([0., 1., 2.], dtype=float16), array([11382, 16310, 13231]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasined/.pyenv/versions/3.10.8/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   51.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.36657185\n",
      "Epoch 3, change: 0.21528719\n",
      "Epoch 4, change: 0.14754980\n",
      "Epoch 5, change: 0.11004162\n",
      "Epoch 6, change: 0.07662492\n",
      "Epoch 7, change: 0.07428241\n",
      "Epoch 8, change: 0.04648536\n",
      "Epoch 9, change: 0.03331655\n",
      "Epoch 10, change: 0.03086124\n",
      "Epoch 11, change: 0.02244124\n",
      "Epoch 12, change: 0.01706062\n",
      "Epoch 13, change: 0.01364512\n",
      "Epoch 14, change: 0.01170810\n",
      "Epoch 15, change: 0.00999285\n",
      "Epoch 16, change: 0.00835102\n",
      "Epoch 17, change: 0.00742854\n",
      "Epoch 18, change: 0.00576953\n",
      "Epoch 19, change: 0.00501298\n",
      "max_iter reached after 51 secondsEpoch 20, change: 0.00436611\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasined/.pyenv/versions/3.10.8/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   51.4s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>#small</th>\n",
       "      <th>#true2size</th>\n",
       "      <th>#large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <td>0.450066</td>\n",
       "      <td>0.348522</td>\n",
       "      <td>0.353792</td>\n",
       "      <td>0.338536</td>\n",
       "      <td>0.486688</td>\n",
       "      <td>3767</td>\n",
       "      <td>9071</td>\n",
       "      <td>4715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy  precision    recall        f1  f1_weighted  #small  \\\n",
       "result  0.450066   0.348522  0.353792  0.338536     0.486688    3767   \n",
       "\n",
       "        #true2size  #large  \n",
       "result        9071    4715  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_samples = X_train.shape[0]\n",
    "\n",
    "model = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1,\n",
    "    solver='sag',\n",
    "    max_iter=20,\n",
    "    multi_class='multinomial',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# model.fit(X_train[num_samples // 5:], y_train[num_samples // 5:])\n",
    "# model.fit(X_train, y_train)\n",
    "random_split_aggr(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# import models\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# importlib.reload(models)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# from models import LogisticClassifier\u001b[39;00m\n\u001b[1;32m      7\u001b[0m clf \u001b[39m=\u001b[39m LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(clf,\n\u001b[1;32m      9\u001b[0m                             X_train,\n\u001b[1;32m     10\u001b[0m                             y_train,\n\u001b[1;32m     11\u001b[0m                             cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m                             scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mf1_macro\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m                             return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m                             n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m cv_results\n\u001b[1;32m     17\u001b[0m \u001b[39m# clf = LogisticClassifier()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# random_split_aggr(clf, X_train, y_train, X_test, y_test)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# random_split_aggr(clf, item_name_train, y_train, item_name_test, y_test)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "# import models\n",
    "# importlib.reload(models)\n",
    "# from models import LogisticClassifier\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "cv_results = cross_validate(clf,\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            cv=5,\n",
    "                            scoring='f1_macro',\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "cv_results\n",
    "\n",
    "# clf = LogisticClassifier()\n",
    "# random_split_aggr(clf, X_train, y_train, X_test, y_test)\n",
    "# random_split_aggr(clf, item_name_train, y_train, item_name_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "# profile = ProfileReport(test_df, minimal=True)\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OrdinalClassifier copied from StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class OrdinalClassifier():\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0] - 1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k: self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for i, y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[i][:, 1])\n",
    "            elif i in clfs_predict:\n",
    "                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\n",
    "                predicted.append(clfs_predict[i - 1][:, 1] -\n",
    "                                 clfs_predict[i][:, 1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[i - 1][:, 1])\n",
    "        return np.vstack(predicted).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model = OrdinalClassifier(LogisticRegression(max_iter=2000))\n",
    "model.fit(train_df_prep.drop('fit', axis=1), train_df_prep['fit'])\n",
    "y_pred = model.predict(test_df_prep.drop('fit', axis=1))\n",
    "\n",
    "evaluate_model(test_df_prep['fit'], y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression with sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto ML with PyCaret (Incorrect Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    setup(\n",
    "        data=train_df_prep,\n",
    "        test_data=test_df_prep,\n",
    "        target='fit',\n",
    "        preprocess=False,\n",
    "        session_id=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('lr', cross_validation=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "input_dim = train_df_prep.drop('fit', axis=1).shape[1]\n",
    "output_dim = 3\n",
    "inputs = torch.tensor(train_df_prep.drop('fit', axis=1).values,\n",
    "                      dtype=torch.float32)\n",
    "labels = torch.tensor(train_df_prep['fit'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "lamda = 1\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels) + lamda * torch.norm(model.linear.weight)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    predicted = model(\n",
    "        torch.tensor(test_df_prep.drop('fit', axis=1).values,\n",
    "                     dtype=torch.float32))\n",
    "    _, predicted = torch.max(predicted.data, 1)\n",
    "    y_pred = predicted.numpy()\n",
    "\n",
    "evaluate_model(test_df_prep['fit'], y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d25da615e19396787fe4ca1ac6e145a6d087d3a93322fbf7b59c4188e44aa5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
