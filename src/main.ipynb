{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'preprocess.DropColumns'>\n",
      "<class 'preprocess.HandleSizeMapping'>\n",
      "<class 'preprocess.OrdinalEncoder'>\n",
      "<class 'preprocess.MeanImputer'>\n",
      "<class 'preprocess.ComputeItemVectors'>\n",
      "Optimizing weights and thresholds, round 1\n",
      "Iteration 0: loss = 14.699899673461914\n",
      "Iteration 100: loss = 7.130328178405762\n",
      "Iteration 200: loss = 4.596743583679199\n",
      "Iteration 300: loss = 3.9746382236480713\n",
      "Optimizing item vectors, round 1\n",
      "Iteration 0: loss = 3.9704017639160156\n",
      "Iteration 100: loss = 3.9702160358428955\n",
      "Iteration 200: loss = 3.9700305461883545\n",
      "Iteration 300: loss = 3.9698455333709717\n",
      "Optimizing weights and thresholds, round 2\n",
      "Iteration 0: loss = 3.969844102859497\n",
      "Iteration 100: loss = 3.7763874530792236\n",
      "Iteration 200: loss = 3.6078364849090576\n",
      "Iteration 300: loss = 3.458024024963379\n",
      "Optimizing item vectors, round 2\n",
      "Iteration 0: loss = 3.456613779067993\n",
      "Iteration 100: loss = 3.4565789699554443\n",
      "Iteration 200: loss = 3.456544876098633\n",
      "Iteration 300: loss = 3.456510543823242\n",
      "Optimizing weights and thresholds, round 3\n",
      "Iteration 0: loss = 3.456510066986084\n",
      "Iteration 100: loss = 3.366342782974243\n",
      "Iteration 200: loss = 3.283543109893799\n",
      "Iteration 300: loss = 3.20788311958313\n",
      "Optimizing item vectors, round 3\n",
      "Iteration 0: loss = 3.2071619033813477\n",
      "Iteration 100: loss = 3.207151412963867\n",
      "Iteration 200: loss = 3.2071409225463867\n",
      "Iteration 300: loss = 3.2071309089660645\n",
      "Optimizing weights and thresholds, round 4\n",
      "Iteration 0: loss = 3.2071306705474854\n",
      "Iteration 100: loss = 3.154967784881592\n",
      "Iteration 200: loss = 3.106529474258423\n",
      "Iteration 300: loss = 3.061662435531616\n",
      "Optimizing item vectors, round 4\n",
      "Iteration 0: loss = 3.0612313747406006\n",
      "Iteration 100: loss = 3.0612270832061768\n",
      "Iteration 200: loss = 3.061222791671753\n",
      "Iteration 300: loss = 3.0612189769744873\n",
      "Optimizing weights and thresholds, round 5\n",
      "Iteration 0: loss = 3.061218738555908\n",
      "Iteration 100: loss = 3.027813673019409\n",
      "Iteration 200: loss = 2.9964938163757324\n",
      "Iteration 300: loss = 2.967163562774658\n",
      "Optimizing item vectors, round 5\n",
      "Iteration 0: loss = 2.9668803215026855\n",
      "Iteration 100: loss = 2.9668781757354736\n",
      "Iteration 200: loss = 2.9668760299682617\n",
      "Iteration 300: loss = 2.966874361038208\n",
      "Optimizing weights and thresholds, round 6\n",
      "Iteration 0: loss = 2.966874361038208\n",
      "Iteration 100: loss = 2.9438984394073486\n",
      "Iteration 200: loss = 2.922178030014038\n",
      "Iteration 300: loss = 2.9016547203063965\n",
      "Optimizing item vectors, round 6\n",
      "Iteration 0: loss = 2.9014551639556885\n",
      "Iteration 100: loss = 2.901454210281372\n",
      "Iteration 200: loss = 2.9014530181884766\n",
      "Iteration 300: loss = 2.90145206451416\n",
      "Optimizing weights and thresholds, round 7\n",
      "Iteration 0: loss = 2.90145206451416\n",
      "Iteration 100: loss = 2.884779453277588\n",
      "Iteration 200: loss = 2.8689069747924805\n",
      "Iteration 300: loss = 2.8537981510162354\n",
      "Optimizing item vectors, round 7\n",
      "Iteration 0: loss = 2.8536510467529297\n",
      "Iteration 100: loss = 2.8536508083343506\n",
      "Iteration 200: loss = 2.8536503314971924\n",
      "Iteration 300: loss = 2.853649616241455\n",
      "Optimizing weights and thresholds, round 8\n",
      "Iteration 0: loss = 2.853649616241455\n",
      "Iteration 100: loss = 2.8410351276397705\n",
      "Iteration 200: loss = 2.82895565032959\n",
      "Iteration 300: loss = 2.8173866271972656\n",
      "Optimizing item vectors, round 8\n",
      "Iteration 0: loss = 2.8172738552093506\n",
      "Iteration 100: loss = 2.8172731399536133\n",
      "Iteration 200: loss = 2.817272901535034\n",
      "Iteration 300: loss = 2.817272424697876\n",
      "Optimizing weights and thresholds, round 9\n",
      "Iteration 0: loss = 2.817272424697876\n",
      "Iteration 100: loss = 2.8074049949645996\n",
      "Iteration 200: loss = 2.797908306121826\n",
      "Iteration 300: loss = 2.7887675762176514\n",
      "Optimizing item vectors, round 9\n",
      "Iteration 0: loss = 2.7886784076690674\n",
      "Iteration 100: loss = 2.788677930831909\n",
      "Iteration 200: loss = 2.788677453994751\n",
      "Iteration 300: loss = 2.788677215576172\n",
      "<class 'preprocess.DropColumns'>\n",
      "<class 'preprocess.OneHotEncoder'>\n",
      "<class 'preprocess.StandardScaler'>\n",
      "<class 'preprocess.TargetEncoder'>\n",
      "<class 'preprocess.DropColumns'>\n",
      "<class 'preprocess.MinMaxScaler'>\n",
      "<class 'preprocess.SelectOutputColumns'>\n",
      "<class 'preprocess.MeanImputer'>\n",
      "<class 'preprocess.MedianImputer'>\n",
      "<class 'preprocess.OneHotEncoder'>\n",
      "<class 'preprocess.AugmentData'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.rcParams['font.family'] = ['serif']\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman']\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "\n",
    "import utils\n",
    "import preprocess\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(preprocess)\n",
    "\n",
    "from utils import fetch_train_data, describe_data, evaluate_model, train_test_split, random_split_aggr\n",
    "from preprocess import *\n",
    "\n",
    "df = fetch_train_data(path='../data/train_data_all_filled.json')\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Preprocess train data\n",
    "prep = Preprocessor()\n",
    "train_df = prep.cleanse(train_df, is_train=True)\n",
    "train_df.dropna(subset=['fit'], inplace=True)\n",
    "\n",
    "prep.pipeline = [\n",
    "    ##\n",
    "    DropColumns(cols=['user_name', 'review', 'review_summary', 'rating']),\n",
    "    HandleSizeMapping(),  # handle size mapping\n",
    "    OrdinalEncoder(cols=['fit', 'item_name', 'cup_size']),  # (necessary)\n",
    "    MeanImputer(\n",
    "        cols=['weight', 'height', 'bust_size', 'cup_size']),  # (necessary)\n",
    "    ComputeItemVectors(),  # compute item vectors\n",
    "    ##\n",
    "    DropColumns(cols=['size_scheme', 'size']),\n",
    "    OneHotEncoder(cols=['size_suffix', 'rented_for', 'body_type']),\n",
    "    StandardScaler(cols=[\n",
    "        'weight', 'height', 'bust_size', 'cup_size', 'item_weight',\n",
    "        'item_height', 'item_bust_size', 'item_cup_size'\n",
    "    ]),\n",
    "    TargetEncoder(cols=['brand', 'category', 'size_main'],\n",
    "                  target_cols=['weight', 'height', 'bust_size', 'cup_size'],\n",
    "                  name='target_encoder'),\n",
    "    DropColumns(cols=['brand', 'category', 'size_main']),\n",
    "    MinMaxScaler(cols=['age', 'price', 'usually_wear']),\n",
    "    SelectOutputColumns(\n",
    "        target='target_encoder'\n",
    "    ),  # append the output of 'target_encoder' to the input of the next transformer\n",
    "    MeanImputer(cols=['age', 'weight', 'height', 'bust_size', 'cup_size']),\n",
    "    MedianImputer(cols=['price', 'usually_wear']),\n",
    "    OneHotEncoder(cols=['item_name']),\n",
    "    AugmentData(target_cols=['weight', 'height', 'bust_size', 'cup_size']),\n",
    "]\n",
    "\n",
    "train_df_prep = train_df.copy()\n",
    "train_df_prep = prep.fit_transform(train_df_prep)\n",
    "\n",
    "X_train = train_df_prep.drop(columns=['fit']).to_numpy(dtype=np.float16)\n",
    "y_train = train_df_prep['fit'].to_numpy(dtype=np.float16)\n",
    "\n",
    "# Preprocess test data\n",
    "test_df = prep.cleanse(test_df)\n",
    "test_df.dropna(subset=['fit'], inplace=True)\n",
    "\n",
    "test_df_prep = test_df.copy()\n",
    "test_df_prep = prep.transform(test_df_prep)\n",
    "\n",
    "X_test = test_df_prep.drop(columns=['fit']).to_numpy(dtype=np.float16)\n",
    "y_test = test_df_prep['fit'].to_numpy(dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.1061 , 0.08   , 0.2585 , ..., 0.     , 0.     , 0.     ],\n",
       "        [0.2932 , 0.02667, 0.3484 , ..., 0.     , 0.     , 0.     ],\n",
       "        [0.1324 , 0.16   , 0.573  , ..., 0.     , 0.     , 0.     ],\n",
       "        ...,\n",
       "        [0.0828 , 0.1067 , 0.2472 , ..., 0.     , 0.     , 0.     ],\n",
       "        [0.144  , 0.1067 , 0.3484 , ..., 0.     , 0.     , 0.     ],\n",
       "        [0.2783 , 0.02667, 0.3484 , ..., 0.     , 0.     , 0.     ]],\n",
       "       dtype=float16),\n",
       " (135132, 4135),\n",
       " array([1., 2., 1., ..., 2., 2., 2.], dtype=float16),\n",
       " (135132,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_train.shape, y_train, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.05716, 0.     , 0.2585 , ..., 0.     , 0.     , 0.     ],\n",
       "        [0.1324 , 0.02667, 0.4382 , ..., 0.     , 0.     , 0.     ],\n",
       "        [0.11536, 0.1866 , 0.4158 , ..., 0.     , 0.     , 0.     ],\n",
       "        ...,\n",
       "        [0.0445 , 0.05334, 0.3035 , ..., 0.     , 0.     , 0.     ],\n",
       "        [0.218  , 0.2134 , 0.2922 , ..., 0.     , 0.     , 0.     ],\n",
       "        [0.1735 , 0.05334, 0.2023 , ..., 0.     , 0.     , 0.     ]],\n",
       "       dtype=float16),\n",
       " (17553, 4135),\n",
       " array([1., 1., 1., ..., 1., 1., 1.], dtype=float16),\n",
       " (17553,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, X_test.shape, y_test, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasined/.pyenv/versions/3.10.8/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.01, multi_class=&#x27;multinomial&#x27;, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01, multi_class=&#x27;multinomial&#x27;, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.01, multi_class='multinomial', n_jobs=-1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_samples = X_train.shape[0]\n",
    "\n",
    "model = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=0.01,\n",
    "    solver='lbfgs',\n",
    "    max_iter=100,\n",
    "    multi_class='multinomial',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "model.fit(X_train[num_samples // 5:], y_train[num_samples // 5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>#small</th>\n",
       "      <th>#true2size</th>\n",
       "      <th>#large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <td>0.611292</td>\n",
       "      <td>0.373559</td>\n",
       "      <td>0.357781</td>\n",
       "      <td>0.355174</td>\n",
       "      <td>0.578665</td>\n",
       "      <td>928</td>\n",
       "      <td>14121</td>\n",
       "      <td>2504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy  precision    recall        f1  f1_weighted  #small  \\\n",
       "result  0.611292   0.373559  0.357781  0.355174     0.578665     928   \n",
       "\n",
       "        #true2size  #large  \n",
       "result       14121    2504  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# import models\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# importlib.reload(models)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# from models import LogisticClassifier\u001b[39;00m\n\u001b[1;32m      7\u001b[0m clf \u001b[39m=\u001b[39m LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(clf,\n\u001b[1;32m      9\u001b[0m                             X_train,\n\u001b[1;32m     10\u001b[0m                             y_train,\n\u001b[1;32m     11\u001b[0m                             cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m                             scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mf1_macro\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m                             return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m                             n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m cv_results\n\u001b[1;32m     17\u001b[0m \u001b[39m# clf = LogisticClassifier()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# random_split_aggr(clf, X_train, y_train, X_test, y_test)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# random_split_aggr(clf, item_name_train, y_train, item_name_test, y_test)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "# import models\n",
    "# importlib.reload(models)\n",
    "# from models import LogisticClassifier\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "cv_results = cross_validate(clf,\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            cv=5,\n",
    "                            scoring='f1_macro',\n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "cv_results\n",
    "\n",
    "# clf = LogisticClassifier()\n",
    "# random_split_aggr(clf, X_train, y_train, X_test, y_test)\n",
    "# random_split_aggr(clf, item_name_train, y_train, item_name_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "# profile = ProfileReport(test_df, minimal=True)\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OrdinalClassifier copied from StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class OrdinalClassifier():\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0] - 1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k: self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for i, y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[i][:, 1])\n",
    "            elif i in clfs_predict:\n",
    "                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\n",
    "                predicted.append(clfs_predict[i - 1][:, 1] -\n",
    "                                 clfs_predict[i][:, 1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[i - 1][:, 1])\n",
    "        return np.vstack(predicted).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "model = OrdinalClassifier(LogisticRegression(max_iter=2000))\n",
    "model.fit(train_df_prep.drop('fit', axis=1), train_df_prep['fit'])\n",
    "y_pred = model.predict(test_df_prep.drop('fit', axis=1))\n",
    "\n",
    "evaluate_model(test_df_prep['fit'], y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression with sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto ML with PyCaret (Incorrect Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    setup(\n",
    "        data=train_df_prep,\n",
    "        test_data=test_df_prep,\n",
    "        target='fit',\n",
    "        preprocess=False,\n",
    "        session_id=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('lr', cross_validation=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "input_dim = train_df_prep.drop('fit', axis=1).shape[1]\n",
    "output_dim = 3\n",
    "inputs = torch.tensor(train_df_prep.drop('fit', axis=1).values,\n",
    "                      dtype=torch.float32)\n",
    "labels = torch.tensor(train_df_prep['fit'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "lamda = 1\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels) + lamda * torch.norm(model.linear.weight)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    predicted = model(\n",
    "        torch.tensor(test_df_prep.drop('fit', axis=1).values,\n",
    "                     dtype=torch.float32))\n",
    "    _, predicted = torch.max(predicted.data, 1)\n",
    "    y_pred = predicted.numpy()\n",
    "\n",
    "evaluate_model(test_df_prep['fit'], y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d25da615e19396787fe4ca1ac6e145a6d087d3a93322fbf7b59c4188e44aa5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
