{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'preprocess.DropColumns'>\n",
      "<class 'preprocess.OrdinalEncoder'>\n",
      "<class 'preprocess.MedianImputer'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from torch import save\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from utils import fetch_train_data\n",
    "from models import *\n",
    "from preprocess import *\n",
    "\n",
    "# Fetch data from course homepage\n",
    "train_df = fetch_train_data(path='../data/train_data_sample.json')\n",
    "\n",
    "# Cleanse data\n",
    "prep = Preprocessor(pipeline=[\n",
    "    DropColumns(cols=[\n",
    "        'user_name', 'review', 'review_summary', 'rating', 'brand', 'category',\n",
    "        'size', 'size_main', 'size_scheme', 'size_suffix', 'price',\n",
    "        'rented_for', 'usually_wear', 'age', 'body_type'\n",
    "    ]),\n",
    "    OrdinalEncoder(cols=['fit', 'cup_size', 'item_name']),\n",
    "    # StandardScaler(cols=['weight', 'height', 'bust_size', 'cup_size']),\n",
    "    MedianImputer(cols=['weight', 'height', 'bust_size', 'cup_size']),\n",
    "])\n",
    "train_df = prep.cleanse(train_df, is_train=True)\n",
    "train_df.dropna(subset=['fit'], inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df = prep.handle_size_mapping(train_df)\n",
    "train_df = prep.fit_transform(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit</th>\n",
       "      <th>item_name</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>bust_size</th>\n",
       "      <th>cup_size</th>\n",
       "      <th>size_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "      <td>170.18</td>\n",
       "      <td>62.595747</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2753</td>\n",
       "      <td>165.10</td>\n",
       "      <td>56.245454</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3756</td>\n",
       "      <td>165.10</td>\n",
       "      <td>68.038855</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2374</td>\n",
       "      <td>170.18</td>\n",
       "      <td>68.038855</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>519</td>\n",
       "      <td>165.10</td>\n",
       "      <td>72.574779</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69917</th>\n",
       "      <td>1</td>\n",
       "      <td>2607</td>\n",
       "      <td>160.02</td>\n",
       "      <td>62.595747</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69918</th>\n",
       "      <td>1</td>\n",
       "      <td>586</td>\n",
       "      <td>175.26</td>\n",
       "      <td>62.595747</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69919</th>\n",
       "      <td>2</td>\n",
       "      <td>3335</td>\n",
       "      <td>165.10</td>\n",
       "      <td>52.163123</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69920</th>\n",
       "      <td>1</td>\n",
       "      <td>3367</td>\n",
       "      <td>165.10</td>\n",
       "      <td>56.699046</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69921</th>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>165.10</td>\n",
       "      <td>62.595747</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69922 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fit  item_name  height     weight  bust_size  cup_size  size_bias\n",
       "0        1        193  170.18  62.595747       34.0       2.0        0.0\n",
       "1        2       2753  165.10  56.245454       32.0       6.0       -2.0\n",
       "2        1       3756  165.10  68.038855       36.0       6.0        2.0\n",
       "3        1       2374  170.18  68.038855       34.0       3.0        0.0\n",
       "4        2        519  165.10  72.574779       34.0       9.0       -1.0\n",
       "...    ...        ...     ...        ...        ...       ...        ...\n",
       "69917    1       2607  160.02  62.595747       32.0       1.0       -2.0\n",
       "69918    1        586  175.26  62.595747       34.0       4.0        0.5\n",
       "69919    2       3335  165.10  52.163123       30.0       4.0       -1.5\n",
       "69920    1       3367  165.10  56.699046       34.0       2.0       -1.5\n",
       "69921    1        577  165.10  62.595747       34.0       3.0        0.5\n",
       "\n",
       "[69922 rows x 7 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing weights and thresholds, round 1\n",
      "Iteration 0: loss = 13.73210620880127\n",
      "Iteration 100: loss = 6.284204006195068\n",
      "Iteration 200: loss = 3.8980579376220703\n",
      "Iteration 300: loss = 3.355808973312378\n",
      "Optimizing item vectors, round 1\n",
      "Iteration 0: loss = 3.352354049682617\n",
      "Iteration 100: loss = 3.3522043228149414\n",
      "Iteration 200: loss = 3.3520548343658447\n",
      "Iteration 300: loss = 3.3519065380096436\n",
      "Optimizing weights and thresholds, round 2\n",
      "Iteration 0: loss = 3.351905107498169\n",
      "Iteration 100: loss = 3.1982333660125732\n",
      "Iteration 200: loss = 3.0697100162506104\n",
      "Iteration 300: loss = 2.958486557006836\n",
      "Optimizing item vectors, round 2\n",
      "Iteration 0: loss = 2.957451105117798\n",
      "Iteration 100: loss = 2.9574224948883057\n",
      "Iteration 200: loss = 2.9573941230773926\n",
      "Iteration 300: loss = 2.9573655128479004\n",
      "Optimizing weights and thresholds, round 3\n",
      "Iteration 0: loss = 2.9573652744293213\n",
      "Iteration 100: loss = 2.891524314880371\n",
      "Iteration 200: loss = 2.8316962718963623\n",
      "Iteration 300: loss = 2.7775232791900635\n",
      "Optimizing item vectors, round 3\n",
      "Iteration 0: loss = 2.7770087718963623\n",
      "Iteration 100: loss = 2.7769992351531982\n",
      "Iteration 200: loss = 2.7769899368286133\n",
      "Iteration 300: loss = 2.776980400085449\n",
      "Optimizing weights and thresholds, round 4\n",
      "Iteration 0: loss = 2.776980400085449\n",
      "Iteration 100: loss = 2.7398996353149414\n",
      "Iteration 200: loss = 2.705655336380005\n",
      "Iteration 300: loss = 2.6740965843200684\n",
      "Optimizing item vectors, round 4\n",
      "Iteration 0: loss = 2.6737942695617676\n",
      "Iteration 100: loss = 2.6737899780273438\n",
      "Iteration 200: loss = 2.673786163330078\n",
      "Iteration 300: loss = 2.6737823486328125\n",
      "Optimizing weights and thresholds, round 5\n",
      "Iteration 0: loss = 2.6737821102142334\n",
      "Iteration 100: loss = 2.6503891944885254\n",
      "Iteration 200: loss = 2.628539562225342\n",
      "Iteration 300: loss = 2.6081526279449463\n",
      "Optimizing item vectors, round 5\n",
      "Iteration 0: loss = 2.6079559326171875\n",
      "Iteration 100: loss = 2.6079537868499756\n",
      "Iteration 200: loss = 2.6079518795013428\n",
      "Iteration 300: loss = 2.607949733734131\n",
      "Optimizing weights and thresholds, round 6\n",
      "Iteration 0: loss = 2.607949733734131\n",
      "Iteration 100: loss = 2.5920355319976807\n",
      "Iteration 200: loss = 2.5770392417907715\n",
      "Iteration 300: loss = 2.5629172325134277\n",
      "Optimizing item vectors, round 6\n",
      "Iteration 0: loss = 2.5627799034118652\n",
      "Iteration 100: loss = 2.562778949737549\n",
      "Iteration 200: loss = 2.5627777576446533\n",
      "Iteration 300: loss = 2.562776565551758\n",
      "Optimizing weights and thresholds, round 7\n",
      "Iteration 0: loss = 2.562776565551758\n",
      "Iteration 100: loss = 2.5513429641723633\n",
      "Iteration 200: loss = 2.5404934883117676\n",
      "Iteration 300: loss = 2.5302019119262695\n",
      "Optimizing item vectors, round 7\n",
      "Iteration 0: loss = 2.530101776123047\n",
      "Iteration 100: loss = 2.5301010608673096\n",
      "Iteration 200: loss = 2.5301003456115723\n",
      "Iteration 300: loss = 2.530099630355835\n",
      "Optimizing weights and thresholds, round 8\n",
      "Iteration 0: loss = 2.530099630355835\n",
      "Iteration 100: loss = 2.521538019180298\n",
      "Iteration 200: loss = 2.5133681297302246\n",
      "Iteration 300: loss = 2.5055737495422363\n",
      "Optimizing item vectors, round 8\n",
      "Iteration 0: loss = 2.505497694015503\n",
      "Iteration 100: loss = 2.5054969787597656\n",
      "Iteration 200: loss = 2.5054967403411865\n",
      "Iteration 300: loss = 2.5054962635040283\n",
      "Optimizing weights and thresholds, round 9\n",
      "Iteration 0: loss = 2.5054962635040283\n",
      "Iteration 100: loss = 2.498873710632324\n",
      "Iteration 200: loss = 2.492525577545166\n",
      "Iteration 300: loss = 2.486440658569336\n",
      "Optimizing item vectors, round 9\n",
      "Iteration 0: loss = 2.4863810539245605\n",
      "Iteration 100: loss = 2.4863808155059814\n",
      "Iteration 200: loss = 2.4863803386688232\n",
      "Iteration 300: loss = 2.486380100250244\n"
     ]
    }
   ],
   "source": [
    "old_vectors = torch.tensor(train_df.groupby('item_name')[[\n",
    "    'weight', 'height', 'bust_size', 'cup_size'\n",
    "]].mean().values,\n",
    "                           dtype=torch.float32)\n",
    "old_deviations = torch.ones_like(old_vectors)\n",
    "\n",
    "\n",
    "class ItemVectorOptimizer:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            w=torch.ones(4, dtype=torch.float32),\n",
    "            b_1=0.0,\n",
    "            b_2=0.0,\n",
    "    ):\n",
    "        # ground truth\n",
    "        self.y = torch.tensor(df['fit'].values, dtype=torch.long)\n",
    "        self.y_1 = (self.y == 0).type(torch.long) - (self.y == 1).type(\n",
    "            torch.long)\n",
    "        self.y_2 = (self.y == 1).type(torch.long) - (self.y == 2).type(\n",
    "            torch.long)\n",
    "        # Loss weights. First, we compute the inverse of the class frequency,\n",
    "        # then we normalize the weights so that they sum to 1.\n",
    "        self.weights = 1 / torch.tensor(\n",
    "            df['fit'].value_counts().sort_index().values, dtype=torch.float32)\n",
    "        self.weights /= torch.sum(self.weights)\n",
    "        self.weights += 2\n",
    "        self.weights = self.weights[self.y]\n",
    "        # size bias\n",
    "        self.bias = torch.tensor(df['size_bias'].values, dtype=torch.float32)\n",
    "        # user vectors\n",
    "        self.u_vec = torch.tensor(\n",
    "            df[['weight', 'height', 'bust_size', 'cup_size']].values,\n",
    "            dtype=torch.float32)\n",
    "        # parent item index for each item\n",
    "        self.pi_idx = torch.tensor(df['item_name'].values, dtype=torch.long)\n",
    "        # item index for each parent item\n",
    "        self.pi_idx_inv = torch.tensor(\n",
    "            df.groupby('item_name').sample(1).sort_index().index.values,\n",
    "            dtype=torch.long)\n",
    "        # (initial) parent item vectors\n",
    "        self.pi_vec = torch.tensor(df.groupby('item_name')[[\n",
    "            'weight', 'height', 'bust_size', 'cup_size'\n",
    "        ]].mean().sort_index().values,\n",
    "                                   dtype=torch.float32)\n",
    "        # (initial) parent item deviations\n",
    "        self.pi_dev = torch.ones_like(self.pi_vec)\n",
    "        # (initial) weights & thresholds\n",
    "        self.w = w\n",
    "        self.b_1 = b_1\n",
    "        self.b_2 = b_2\n",
    "        # item vectors\n",
    "        self.i_vec = self.pi_vec[self.pi_idx] + (self.bias *\n",
    "                                                 self.pi_dev[self.pi_idx].T).T\n",
    "        # fitness scores\n",
    "        self.f = (self.i_vec - self.u_vec) @ self.w\n",
    "\n",
    "    # Projected Gradient Descent 1\n",
    "    def optimize_weights_thresholds(self, lr=0.01, max_iter=1000):\n",
    "        for i in range(max_iter + 1):\n",
    "            # calculate gradients\n",
    "            sigma_1 = torch.sigmoid(self.y_1 * (self.b_1 - self.f))\n",
    "            sigma_2 = torch.sigmoid(self.y_2 * (self.b_2 - self.f))\n",
    "            grad_w = torch.mean(\n",
    "                ((self.y_1 * (1 - sigma_1) + self.y_2 *\n",
    "                  (1 - sigma_2))[:, None] *\n",
    "                 (self.i_vec - self.u_vec)) * self.weights[:, None],\n",
    "                dim=0)\n",
    "            grad_b_1 = torch.mean(-self.y_1 * (1 - sigma_1) * self.weights)\n",
    "            grad_b_2 = torch.mean(-self.y_2 * (1 - sigma_2) * self.weights)\n",
    "            # update weights and project to non-negative orthant\n",
    "            self.w -= lr * grad_w\n",
    "            self.w = torch.max(self.w, torch.zeros_like(self.w))\n",
    "            # update thresholds\n",
    "            self.b_1 -= lr * grad_b_1\n",
    "            self.b_2 -= lr * grad_b_2\n",
    "            # update fitness scores and loss\n",
    "            self.f = (self.i_vec - self.u_vec) @ self.w\n",
    "            self.loss = torch.mean(\n",
    "                (-torch.log(sigma_1) - torch.log(sigma_2)) * self.weights)\n",
    "            if i % 100 == 0:\n",
    "                print(f'Iteration {i}: loss = {self.loss}')\n",
    "\n",
    "    # Projected Gradient Descent 2\n",
    "    def optimize_item_vectors(self, lr=0.01, max_iter=1000):\n",
    "        for i in range(max_iter + 1):\n",
    "            # calculate gradients\n",
    "            sigma_1 = torch.sigmoid(self.y_1 * (self.b_1 - self.f))\n",
    "            sigma_2 = torch.sigmoid(self.y_2 * (self.b_2 - self.f))\n",
    "            grad_i_vec = (self.y_1 * (1 - sigma_1) + self.y_2 *\n",
    "                          (1 - sigma_2))[:, None] * self.w\n",
    "            grad_pi_vec = grad_i_vec[self.pi_idx_inv]\n",
    "            grad_pi_dev = (self.bias[:, None] * grad_i_vec)[self.pi_idx_inv]\n",
    "            # update parent item vectors and project deviations to non-negative orthant\n",
    "            self.pi_vec -= lr * grad_pi_vec\n",
    "            self.pi_dev -= lr * grad_pi_dev\n",
    "            self.pi_dev = torch.max(self.pi_dev, torch.zeros_like(self.pi_dev))\n",
    "            # update item vectors, fitness scores and loss\n",
    "            self.i_vec = self.pi_vec[\n",
    "                self.pi_idx] + self.bias[:, None] * self.pi_dev[self.pi_idx]\n",
    "            self.f = (self.i_vec - self.u_vec) @ self.w\n",
    "            self.loss = torch.mean(\n",
    "                (-torch.log(sigma_1) - torch.log(sigma_2)) * self.weights)\n",
    "            if i % 100 == 0:\n",
    "                print(f'Iteration {i}: loss = {self.loss}')\n",
    "\n",
    "    def predict_proba(self):\n",
    "\n",
    "        prob_2 = torch.sigmoid(self.f - self.b_2)\n",
    "        prob_1 = torch.sigmoid(self.f - self.b_1) - prob_2\n",
    "        prob_0 = 1 - prob_1 - prob_2\n",
    "        return torch.stack([prob_0, prob_1, prob_2], dim=1)\n",
    "\n",
    "    def predict(self):\n",
    "        return torch.argmax(self.predict_proba(), dim=1)\n",
    "\n",
    "    def accuracy(self):\n",
    "        return torch.mean((self.predict() == self.y).type(torch.float32))\n",
    "\n",
    "    def f1_score(self):\n",
    "        from sklearn.metrics import f1_score\n",
    "        return f1_score(self.y, self.predict(), average='macro')\n",
    "\n",
    "\n",
    "weights = torch.tensor([1., 1., 1., 1.], dtype=torch.float32)\n",
    "threshold_1 = torch.tensor(-1, dtype=torch.float32)\n",
    "threshold_2 = torch.tensor(1, dtype=torch.float32)\n",
    "\n",
    "optim = ItemVectorOptimizer(train_df,\n",
    "                            w=weights,\n",
    "                            b_1=threshold_1,\n",
    "                            b_2=threshold_2)\n",
    "for i in range(1, 10):\n",
    "    print(f'Optimizing weights and thresholds, round {i}')\n",
    "    optim.optimize_weights_thresholds(lr=1e-3 / i, max_iter=300)\n",
    "    print(f'Optimizing item vectors, round {i}')\n",
    "    optim.optimize_item_vectors(lr=1e-5 / i, max_iter=300)\n",
    "    # print(optim.w, optim.b_1.item(), optim.b_2.item())\n",
    "    # print((optim.pi_vec - old_vectors).mean().item(),\n",
    "    #       (optim.pi_dev - old_deviations).mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3222197378555875"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(optim.predict(), return_counts=True)\n",
    "optim.f1_score()\n",
    "# optim.accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d25da615e19396787fe4ca1ac6e145a6d087d3a93322fbf7b59c4188e44aa5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
